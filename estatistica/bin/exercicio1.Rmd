---
title: "UFPR PPGEcon 2024"
subtitle: "Estatística (Prof. Adalto Acir Althaus Jr.): exercícios"
author: "Alceu Nascimento, João Maurício Rolim e Tiago Soares"
date: "2024-04-21"
output:
  html_document:
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    toc_depth: 5
header-includes:
- \usepackage{fontspec}
- \setmainfont{Arial}
---

# **Exercícios 01**

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# definir notação cientifica off em numeros menores que 1.000.000.000.000 ----
# definir qual o tipo do separador (ponto ou virgula) ----
options(scipen = 10, digits = 10, OutDec = ".")
# basic ----
library(writexl)               # Salva as tabelas elaboradas em formato .xls
library(readxl)                # Reads Microsoft Excel spreadsheets.
library(knitr)                 # tabelas kable
library(kableExtra)            # Build common complex HTML tables and manipulate table styles.
library(readr)                 # A fast and friendly way to read tabular data into R.
library(MASS)                  # visualiza decimal em fracoes
library(xtable)                # transforma tabela Excel para Latex
library(ggplot2)               # graficos
library(gridExtra)
library(grid)  # Para configurações adicionais de grid
# data manipulation ----
library(tidyverse)             # Inclui dplyr, forcats, ggplto2, lubridate, purrr, stringr, tibble, tidyr
library(broom)                 # Converte saídas de modelos estatísticos em tibbles
library(dbplyr)                # Interface dplyr para bancos de dados
library(lubridate)             # Simplifica trabalho com datas e horas
library(janitor)
# statistics ----
library(stargazer)             # analise estatistica
library(skimr)                 # Compact and flexible summaries of data, a frictionless, pipeable approach to dealing with summary statistic
library(broom)                 # Convert statistical analysis objects into tidy data frames.
library(lmtest)                # Hypothesis testing for linear regression models.
library(modelsummary)          # faz um grafico de intervalo bom
library(strucchange)           # analisar quebras estruturais (Chow Test)
library(mctest)                # teste para multicolinearidad
library(performance)           # Analisa regressão 
library(broom)                 # takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy tibbles.
library(Hmisc)
library(pastecs)
library(nortest)               # Anderson-Darling Test for normality
```

------------------------------------------------------------------------

## **Item 1**

A planilha DEMO traz informações de 1.000 respondentes quanto à sua
idade em anos, o seu estado civil (1- casado , 0- não casado), quanto
tempo (em anos) vive no endereço atual, sua renda anual (em milhares de
reais), o preço do carro principal (em milhares de reais), sua
escolaridade (1- primeiro grau, 2- segundo grau, 3- terceiro grau, 4-Pós
graduação especialização, 5-mestrado/doutorado), quanto tempo, em anos,
está no emprego atual (t_emp_atual), se é (1) ou não (0) aposentado, o
sexo (m- masc e f- femin) e sua satisfação no trabalho (de 1- Nada
satisfeito a 5- Muito satisfeito).

### Apuração:

```{r get data 1, options}
df1 <- read_csv2("C:/Users/DELL/OneDrive/R/Rprojetos/ufpr_ppgecon/estatistica/data/Exerc_1_descritiva_dataset1.csv")
df1
```

```{r analize data 1, options}
# medidas estatisticas
summary(df1)
```

### Questões:  

#### a) Classifique cada variável em ESCALAR, ORDINAL ou NOMINAL

\  

| variável      | qualitativa | quantitativa |
|---------------|-------------|--------------|
| idade         |             | ESCALAR      |
| est_civil     | NOMINAL     |              |
| endereco      |             | ESCALAR      |
| renda         |             | ESCALAR      |
| carro         |             | ESCALAR      |
| escolaridade  | ORDINAL     |              |
| t_empr_atual  |             | ESCALAR      |
| aposentado    | NOMINAL     |              |
| sexo          | NOMINAL     |              |
| satisf_trabal | ORDINAL     |              |


\  


#### b) Represente as variáveis categóricas graficamente para resumir as informações da melhor maneira possível

Resp:

```{r item 1 b, options}
library(ggplot2)
# Transformando variáveis categóricas em fatores
df1$est_civil <- factor(df1$est_civil)
df1$aposentado <- factor(df1$aposentado)
df1$sexo <- factor(df1$sexo)
df1$escolaridade <- factor(df1$escolaridade)
df1$satisf_trabal <- factor(df1$satisf_trabal)
```

```{r item 1 b graficos}
# Definindo os rótulos personalizados
labels <- c("solteiro", "casado")

# Criando uma nova variável com os rótulos personalizados
df1$est_civil_label <- ifelse(df1$est_civil == 0, "solteiro", "casado")

# Gráfico de Pizza para 'est_civil' com rótulos modificados
ggplot(df1, aes(x = "", fill = est_civil_label)) +
  geom_bar(width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("solteiro" = "lightblue", "casado" = "grey")) + 
  labs(title = "Distribuição do Estado Civil")

# Definindo os rótulos personalizados
labels_aposentado <- c("não", "sim")

# Criando uma nova variável com os rótulos personalizados para 'aposentado'
df1$aposentado_label <- ifelse(df1$aposentado == 0, "não", "sim")

# Gráfico de Pizza para 'aposentado' com rótulos modificados
ggplot(df1, aes(x = "", fill = aposentado_label)) +
  geom_bar(width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("não" = "lightblue", "sim" = "grey")) + 
  labs(title = "Distribuição de Aposentados")

# Definindo os rótulos personalizados
labels_sexo <- c("feminino", "masculino")

# Criando uma nova variável com os rótulos personalizados para 'sexo'
df1$sexo_label <- ifelse(df1$sexo == "f", "feminino", "masculino")

# Gráfico de Pizza para 'sexo' com rótulos modificados
ggplot(df1, aes(x = "", fill = sexo_label)) +
  geom_bar(width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c("feminino" = "lightblue", "masculino" = "grey")) + 
  labs(title = "Distribuição do Sexo")


```

\  


#### c) Para as variáveis escalares faça um resumo de todas as medidas estudadas (média, mediana, desvio-padrão, etc)

Resp:

```{r item 1 estatisticas, options}
df1_escalares <- df1 |> 
  select(idade, endereco, renda, carro, t_empr_atual)
pastecs::stat.desc(df1_escalares)
```

\  

#### d) Examine a possibilidade das variáveis possuirem distribuição normal de probabilidades

Resp: O teste de normalidade executado indica que a distrição dos dados não apresenta normalidade.    


```{r qq grid, options}
# Função para criar um gráfico Q-Q com ggplot2
make_qqplot <- function(data, var_name) {
  ggplot(data, aes(sample = .data[[var_name]])) +
    stat_qq() +
    stat_qq_line(colour = "red") +
    ggtitle(paste("Q-Q Plot de", var_name)) +
    theme_minimal()
}

# Lista de variáveis numéricas (excluindo variáveis categóricas)
var_names <- c("idade", "renda", "carro", "t_empr_atual")

# Criar uma lista de gráficos Q-Q para cada variável
plots <- lapply(var_names, function(v) make_qqplot(df1, v))

# Organizar os gráficos em uma grade de 2 colunas
grid.arrange(grobs = plots, ncol = 2)
```

```{r hist grid, options}
# Função para criar um histograma com curva de densidade
make_histogram <- function(data, var_name) {
  # Verifica se a variável é numérica; caso contrário, retorna NULL
  if (!is.numeric(data[[var_name]])) {
    return(NULL)
  }
  
  p <- ggplot(data, aes_string(x = var_name)) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = "gray", alpha = 0.7) +
    geom_density(color = "red", size = 1) +
    labs(title = paste("Histograma:", var_name),
         x = var_name,
         y = "Densidade") +
    theme_minimal()
  return(p)
}

# Lista de variáveis numéricas
var_names <- c("idade", "renda", "carro", "t_empr_atual")  

# Criar uma lista de gráficos para cada variável numérica
plots <- lapply(var_names, function(v) make_histogram(df1, v))
plots <- plots[!sapply(plots, is.null)]

# Organizar os gráficos em uma grade de 2 colunas
grid.arrange(grobs = plots, ncol = 2)
```


```{r item 1 shapiro wilk test, options}
# Aplicar o teste de Shapiro-Wilk a todas as colunas numéricas do df1
shapiro_test_results <- lapply(df1, function(x) {
  if (is.numeric(x)) {
    shapiro.test(x)
  } else {
    NULL
  }
})

# Filtrar os resultados para remover os valores NULL (colunas não numéricas)
shapiro_test_results <- Filter(Negate(is.null), shapiro_test_results)

# Extrair os resumos dos resultados
shapiro_test_summaries <- sapply(shapiro_test_results, tidy)

# Exibir os resultados
shapiro_test_summaries

```
\  

------------------------------------------------------------------------

## **Item 2**

Ao lado são apresentados dados de gastos per capita, em milhares de
dólares, para cada estado americano em 20xx.

### Apuração:

```{r get data 2, options}
df2 <- read_csv2("C:/Users/DELL/OneDrive/R/Rprojetos/ufpr_ppgecon/estatistica/data/Exerc_1_descritiva_dataset2.csv")
df2
```

### Questões:  

#### a) Faça um resumo das estatísticas descritivas desses dados  

```{r analize data 2, options}
pastecs::stat.desc(df2$gasto)
```

\  

#### b) Decida se os dados apresentados podem estar aproximadamente normalmente distribuídos

```{r normality tests 2, options}
# Aplicar o teste de Shapiro-Wilk a todas as colunas numéricas do df1
shapiro_test_results <- lapply(df2, function(x) {
  if (is.numeric(x)) {
    shapiro.test(x)
  } else {
    NULL
  }
})

# Filtrar os resultados para remover os valores NULL (colunas não numéricas)
shapiro_test_results <- Filter(Negate(is.null), shapiro_test_results)

# Extrair os resumos dos resultados
shapiro_test_summaries <- sapply(shapiro_test_results, tidy)

# Exibir os resultados
shapiro_test_summaries
```

```{r item 2b grafico, options}
p2 <- ggplot(df2, aes(x = gasto)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "gray", alpha = 0.7) +
  geom_density(color = "red", size = 1) +
  labs(title = "Histograma: Distribuição de Gastos",
       x = "Gasto",
       y = "Densidade") +
  theme_minimal()
p2
```

\  

Os dados da variável `gasto` não apresentam distribuição normal.

\ 
------------------------------------------------------------------------

## **Item 3**

Suponha que o volume de negócios diários comercializados na Bolsa de
Nova York (NYSE) seja uma variável normalmente distribuída com média de
1,8 bilhão e desvio-padrão de 0,15 bilhão.

### Apuração:

```{r get data 3, options}
# parametros
mean3 <- 1.8
sd3 <- 0.15
```

```{r analize data 3, options}

# Calculando as probabilidades 
## Usando 'pnorm': probabilidade de ser menor ou igual a um valor (dist normal)
prob_a <- pnorm(1.5, mean = mean3, sd = sd3)
prob_b <- 1 - pnorm(2, mean = mean3, sd = sd3)
prob_c <- pnorm(1.9, mean = mean3, sd = sd3) - pnorm(1.7, mean = mean3, sd = sd3)

# Gerando um gráfico
## Criando um data frame para o gráfico
x <- seq(mean3 - 4*sd3, mean3 + 5*sd3, length.out = 1000)
y <- dnorm(x, mean = mean3, sd = sd3)
data <- data.frame(x, y)

## Gerando o gráfico
ggplot(data, aes(x, y)) +
  geom_line() +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean3, sd = sd3), 
    geom = "area", 
    fill = "blue", 
    xlim = c(1, 1.5), 
    alpha = 0.3) +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean3, sd = sd3), 
    geom = "area", 
    fill = "red", 
    xlim = c(2, 2.5), 
    alpha = 0.3) +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean3, sd = sd3), 
    geom = "area", 
    fill = "green", 
    xlim = c(1.7, 1.9), 
    alpha = 0.3) +
  labs(title = 'Volume de Negócios na NYSE',
       subtitle = 'Assumindo uma distribuição normal',
       x = 'Volume de Negócios (bilhões)',
       y = 'Probabilidade') +
  theme_bw()
```

### Questões:

Para um dia aleatoriamente escolhido, qual a probabilidade do volume
estar: \

#### a) abaixo de 1,5 bilhão?  

Resp: `r prob_a`

#### b) acima de 2 bilhões? 

Resp: `r prob_b`  

#### c) entre 1,7 e 1,9 bilhão? 

Resp: `r prob_c`

\  

------------------------------------------------------------------------

## **Item 4**

Uma análise estatística de 1.000 chamadas telefônicas de longa distância
originadas dos escritórios da Bricks and Clicks Computer Corporation
indicam que a duração dessas chamadas estão **normalmente
distribuídas**. Sendo a média e o desvio-padrão da duração das chamadas
240 segundos e 40 segundos, respectivamente.

### Apuração:

```{r get data 4, options}
mean4 <- 240
sd4 <- 40
```

```{r analize data 4, options}

```

### Questões:
\  

#### a) Calcule a probabilidade de uma chamada durar menos de 180 segundos. 

```{r item 4 a, options}
# set parameter 
valor4a <- 180

# Calculando a probabilidade de uma chamada durar menos de 180 segundos
probabilidade <- pnorm(valor4a, mean = mean4, sd = sd4)
probabilidade
```
\  

#### b) Qual a probabilidade de uma chamada durar entre 200 e 300 segundos? 

```{r item 4 b, options}
# Definindo os limites
limite_inferior4b = 200
limite_superior4b = 300

# Calculando as probabilidades acumuladas
probabilidade_superior = pnorm(limite_superior4b, mean = mean4, sd = sd4)
probabilidade_inferior = pnorm(limite_inferior4b, mean = mean4, sd = sd4)

# Diferença para encontrar a probabilidade entre 200 e 300 segundos
probabilidade_entre = probabilidade_superior - probabilidade_inferior
probabilidade_entre
```

\  

#### c) Um empregado realizou diversas chamadas com duração acima de 350 segundos. Você pode aceitar que esse é um fato casual?

```{r item 4 c, options}
# Definindo o valor crítico
valor_critico4c = 350

# Calculando a probabilidade de uma chamada durar menos ou igual a 350 segundos
probabilidade_menor_igual_350 = pnorm(valor_critico4c, mean = mean4, sd = sd4)

# Calculando a probabilidade de uma chamada durar mais que 350 segundos
probabilidade_maior_350 = 1 - probabilidade_menor_igual_350
probabilidade_maior_350
```

Este não é um fato casual, dada o valor extremamente baixo do p-valor.
Trata-se de evento muito improvável.

------------------------------------------------------------------------

## **Item 5**

Uma pesquisa realizada entre instituições financeiras da América Latina
apresentou os resultados descritos na tabela abaixo. Você diria que
existe associação entre o tempo de atuação e o número de clientes?

### Apuração:

```{r get data 5, options}
df5 <- read_csv2("C:/Users/DELL/OneDrive/R/Rprojetos/ufpr_ppgecon/estatistica/data/Exerc_1_descritiva_dataset5.csv")
df5
```

```{r analize data 5, options}
df5 <- clean_names(df5)
```

### Questões:  

#### a) Construa o diagrama de dispersão dos dados. 

```{r item 5 a, options}
df5 <- df5 |> select("tempo_de_atuacao", "numero_de_clientes")
# Criando o diagrama de dispersão
ggplot(df5, aes(x = tempo_de_atuacao, y = numero_de_clientes)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # Adiciona uma linha de tendência linear
  labs(title = "Diagrama de Dispersão: Tempo de Atuação vs Número de Clientes",
       x = "Tempo de Atuação (anos)",
       y = "Número de Clientes") +
  theme_bw()
```

\  

#### b) Calcule a covariância e o coeficiente de correlação.

```{r item 5 b, options}
# Calculando a covariância
covariancia <- cov(df5$tempo_de_atuacao, df5$numero_de_clientes)

# Calculando o coeficiente de correlação de Pearson
correlacao <- cor(df5$tempo_de_atuacao, df5$numero_de_clientes)

# Exibindo os resultados
list(covariancia = covariancia, correlacao = correlacao)
```

A covariância positiva de 410.15 indica que, em geral, um aumento no
tempo de atuação está associado a um aumento no número de clientes,
sugerindo que as duas variáveis tendem a variar juntas na mesma direção.

O coeficiente de correlação de aproximadamente 0.921 é muito alto e
positivo, indicando uma forte correlação positiva entre o tempo de
atuação e o número de clientes. Isso sugere que instituições com mais
anos de atuação tendem a ter mais clientes, o que pode refletir um
estabelecimento mais sólido no mercado.

Esses resultados indicam que existe uma associação significativa entre o
tempo de atuação das instituições financeiras e o número de seus
clientes, sugerindo que não é apenas um resultado casual, mas uma
tendência clara.

------------------------------------------------------------------------

## **Item 6**

Os preços de fechamento de diversos ativos negociados na BOVESPA
aparecem listados na planilha portfolio.

### Apuração:

```{r get data 6, options}
df6 <- read_csv2("C:/Users/DELL/OneDrive/R/Rprojetos/ufpr_ppgecon/estatistica/data/Exerc_1_descritiva_dataset6.csv")
df6 <- df6 %>%
  mutate(date = dmy(date))
df6
```

```{r analize data 6, options}
# Calculando os retornos diários para cada ticker
df6_returns <- df6 %>%
  mutate(across(where(is.numeric), ~ . / lag(.) - 1)) %>%
  na.omit()
```

### Questões:  

#### a) Se você fosse comprar somente um dos papéis dentre os listados, qual seria a melhor escolha com base no período analisado?  

```{r item 6 a, options}
# Calculando o retorno acumulado e ordenando do maior para o menor
retorno_acumulado <- df6 %>%
  arrange(date) %>%
  summarise(across(where(is.numeric), ~ (last(.) - first(.)) / first(.))) %>%
  pivot_longer(everything(), names_to = "Ticker", values_to = "Return") %>%
  arrange(desc(Return))  # Ordena os retornos de forma decrescente

# Visualizando o retorno acumulado
retorno_acumulado
```

```{r item 6 a cont1, options}
# Calculando a volatilidade
volatilidade <- df6 %>%
  summarise(across(-date, sd, na.rm = TRUE)) %>%
  pivot_longer(cols = everything(), names_to = "Ticker", values_to = "Volatility")
volatilidade <- volatilidade %>%
  arrange(Volatility)

volatilidade
```

```{r sharpie, options}
# Rácio de Sharpe, assumindo taxa livre de risco = 0
sharpe_ratio <- df6 %>%
  summarise(across(-date, ~ (mean(., na.rm = TRUE) / sd(., na.rm = TRUE)))) %>%
  pivot_longer(cols = everything(), names_to = "Ticker", values_to = "Sharpe_Ratio")

sharpe_ratio
```

\  

#### b) Calcule o risco e o retorno de uma carteira formada com 50% de PETR4 e 50% de VALE5. Simule o resultado para diversos níveis de correlação. 

**Retorno da Carteira**: O retorno esperado de uma carteira é uma média ponderada dos retornos esperados dos ativos individuais, ponderados pelas proporções dos ativos na carteira.

$$
E(r_p) = w_1 E(r_1) + w_2 E(r_2) = w_1 E(r_1) + (1 - w_1) E(r_2)
$$

**Risco da Carteira (Volatilidade)**: O risco de uma carteira é calculado usando a fórmula da variância de uma combinação de dois ativos. Esta fórmula incorpora a volatilidade de cada ativo e a correlação entre eles.

$$
\sigma_p^2 = w_1^2 \sigma_1^2 + w_2^2 \sigma_2^2 + 2w_1w_2 \rho_{1,2} \sigma_a\sigma_2
$$


Para cada um deles temos as seguintes fórmulas:

A. Retorno da Carteira:

$$
E(r_p) = 0.5 E(r_{\text{PETR4}}) + 0.5 E(r_{\text{VALE5}})
$$
\  

Onde $0.5$ são os pesos de PETR4 e VALE5 na carteira.  

B. Risco da Carteira (desvio padrão):

$$
\sigma_p^2 = 
(0.5^2  \sigma_{\text{PETR4}}^2) + 
(0.5^2 \sigma_{\text{VALE5}}^2) +
2
(0.5)(0.5) \sigma_{\text{PETR4}} \sigma_{\text{VALE5}} \rho_{\text{PETR4,VALE5}} 
$$

\  

onde $\sigma_{\text{PETR4}}$ e $\sigma_{\text{VALE5}}$ são os desvios padrões dos retornos de PETR4 e VALE5, e $\rho$ é a correlação entre eles.

\  

Vamos calcular o retorno e risco da carteira para diferentes níveis de
correlação entre -1 e 1:

```{r item 6 b grafico1, options}

# Criando o gráfico de linhas
grafico_retornos <- ggplot(df6_returns, aes(x = date)) +
  geom_line(aes(y = PETR4, color = "PETR4"), linetype = "solid") +
  geom_line(aes(y = VALE5, color = "VALE5"), linetype = "dashed") +
  labs(title = "Retornos Diários de PETR4 e VALE5",
       x = "Data",
       y = "Retorno Diário",
       color = "Ticker") +
  theme_bw()

# Exibindo o gráfico
print(grafico_retornos)

```

```{r item 6 b, options}
# Média e desvio padrão dos retornos
mean_PETR4 <- mean(df6_returns$PETR4, na.rm = TRUE)
mean_VALE5 <- mean(df6_returns$VALE5, na.rm = TRUE)
sd_PETR4 <- sd(df6_returns$PETR4, na.rm = TRUE)
sd_VALE5 <- sd(df6_returns$VALE5, na.rm = TRUE)

# Pesos
w_PETR4 <- 0.5
w_VALE5 <- 0.5

# Simulando para diferentes níveis de correlação
correlations <- seq(-1, 1, by = 0.1)
portfolio_stats <- sapply(correlations, function(rho) {
  portfolio_return <- w_PETR4 * mean_PETR4 + w_VALE5 * mean_VALE5
  portfolio_risk <- sqrt(w_PETR4^2 * sd_PETR4^2 + w_VALE5^2 * sd_VALE5^2 + 2 * w_PETR4 * w_VALE5 * sd_PETR4 * sd_VALE5 * rho)
  c(Return = portfolio_return, Risk = portfolio_risk)
})

# Convertendo os resultados em um dataframe mais amigável
portfolio_results <- as.data.frame(t(portfolio_stats))
colnames(portfolio_results) <- c("Return", "Risk")
portfolio_results$Correlation <- correlations

# Mostrando os resultados
portfolio_results
```

\  

#### c) Encontre os papéis com menor correlação. 

```{r item 6 c, options}
# Calculando a matriz de correlação
correlation_matrix <- cor(df6_returns[, -1], use = "complete.obs")  # Ignora a primeira coluna 'date'

# Transformando a matriz em um formato tabular e filtrando para encontrar os menores valores positivos
correlation_df <- as.data.frame(as.table(correlation_matrix))
colnames(correlation_df) <- c("Ticker1", "Ticker2", "Correlation")

# Removendo correlações de uma ação com ela mesma e duplicatas
correlation_df <- correlation_df %>%
  filter(Ticker1 != Ticker2) %>%
  filter(Correlation < 1) %>%
  arrange(Correlation)

# Encontrando o par com a menor correlação
lowest_correlation <- correlation_df %>%
  filter(Correlation == min(Correlation[Correlation > -1 & Correlation < 1]))

# Mostrando o par de ações com a menor correlação
lowest_correlation
```

\  

#### d) Com base nesses dois papéis, qual percentual do seu capital você aplicaria em cada ação para obter a carteira de menor risco? 

Formulação do Problema

Para duas ações $A$ e $B$, a variância $\sigma_p^2$ da carteira pode ser expressa como:

\  

$$ \sigma_p^2 = w_A^2\sigma_A^2 + w_B^2\sigma_B^2 + 2w_Aw_B\sigma_A\sigma_B\rho_{A,B} $$

Onde:

-   $w_A$ e $w_B$ são os pesos das ações $A$ e $B$ na carteira, respectivamente.
-   $\sigma_A$ e $\sigma_B$ são os desvios padrão dos retornos de $A$ e $B$.
-   $\rho_{A,B}$ é a correlação entre os retornos de $A$ e $B$.

Os pesos são determinados de forma que $w_A + w_B = 1$ e que minimizem $\sigma_p^2$.

A solução para os pesos que minimizam a variância da carteira é dada por:

$$ w_A = \frac{\sigma_B^2 - \rho_{A,B}\sigma_A\sigma_B}{\sigma_A^2 + \sigma_B^2 - 2\rho_{A,B}\sigma_A\sigma_B} $$
$$ w_B = 1 - w_A $$

\  

Apurando:  

```{r item 6 d, options}
# Primeiro, determine os desvios padrão das duas ações identificadas
# Vamos assumir que os tickers são 'TickerA' e 'TickerB' do resultado anterior
tickerA <- lowest_correlation$Ticker1[1]
tickerB <- lowest_correlation$Ticker2[1]

sd_A <- sd(df6_returns[[tickerA]], na.rm = TRUE)
sd_B <- sd(df6_returns[[tickerB]], na.rm = TRUE)

# Correlação entre A e B
rho_AB <- lowest_correlation$Correlation[1]

# Calculando os pesos para minimizar a variância da carteira
w_A <- (sd_B^2 - rho_AB * sd_A * sd_B) / (sd_A^2 + sd_B^2 - 2 * rho_AB * sd_A * sd_B)
w_B <- 1 - w_A

# Exibindo os resultados
list(w_A = w_A, w_B = w_B)
```

\  

#### e) Os retornos parecem seguir uma curva normal?

```{r item 6 e, options}
# Transformando o dataframe para formato longo
df6_long <- df6_returns %>%
  pivot_longer(cols = -date, names_to = "Ticker", values_to = "Returns") %>%
  drop_na()

# Plotando as curvas de densidade sobrepostas para cada ticker
density_plot <- ggplot(df6_long, aes(x = Returns, color = Ticker, group = Ticker)) +
  geom_density() +  # Removido o preenchimento e a transparência
  labs(title = "Sobreposição das Curvas de Densidade de Retornos por Ticker",
       x = "Retornos",
       y = "Densidade") +
  theme_minimal() +
  theme(legend.title = element_text(size = 12),
        legend.text = element_text(size = 10))

# Exibindo o gráfico
print(density_plot)
```


```{r item 6 e cont2, options}
# Aplicando o teste de Shapiro-Wilk para normalidade para cada ticker
shapiro_test_results <- lapply(df6_returns[, -1], shapiro.test)
shapiro_test_summaries <- sapply(shapiro_test_results, tidy)
shapiro_test_summaries
```

Fim.
